import torch
import torch.nn.functional as F
import numpy as np
import pandas as pd
from torch_geometric.data import HeteroData
from torch_geometric.nn import SAGEConv, to_hetero, HeteroConv
from torch_geometric.loader import LinkNeighborLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# Set a random seed for reproducibility
torch.manual_seed(42)
np.random.seed(42)

# --- 1. DATA SIMULATION & SETUP (Replace with your actual data loading) ---

# Constraints from the problem
NUM_IMPRESSIONS = 2500000  # 2.5M rows
NUM_CMS = 111000          # ~111k unique CMs
NUM_OFFERS = 580          # ~580 unique offers
CM_FEAT_DIM = 10
OFFER_FEAT_DIM = 10

print("Simulating data...")

# A. Simulate Card Member (CM) and Offer Features
cm_features = np.random.rand(NUM_CMS, CM_FEAT_DIM).astype(np.float32)
offer_features = np.random.rand(NUM_OFFERS, OFFER_FEAT_DIM).astype(np.float32)

# B. Simulate Impression Data (Edges)
# cust_xref_id (0 to NUM_CMS-1), offer_id (0 to NUM_OFFERS-1), depnt_var (0 or 1)
df_impressions = pd.DataFrame({
    'cust_xref_id': np.random.randint(0, NUM_CMS, NUM_IMPRESSIONS),
    'offer_id': np.random.randint(0, NUM_OFFERS, NUM_IMPRESSIONS),
    'depnt_var': np.random.choice([0, 1], size=NUM_IMPRESSIONS, p=[0.95, 0.05]) # Low redemption rate
})

# C. Get Mappings and Edge Tensors
src_indices = df_impressions['cust_xref_id'].values
dst_indices = df_impressions['offer_id'].values
edge_label = torch.tensor(df_impressions['depnt_var'].values, dtype=torch.float32)

# PyG requires edge_index in (2, num_edges) format
edge_index = torch.tensor([src_indices, dst_indices], dtype=torch.long)

# --- 2. BUILD THE HETEROGENEOUS GRAPH (PyG HeteroData) ---

data = HeteroData()

# Node features
data['customer'].x = torch.tensor(cm_features, dtype=torch.float32)
data['offer'].x = torch.tensor(offer_features, dtype=torch.float32)

# Edges (Relationship: customer -> impressed_on -> offer)
data['customer', 'impressed_on', 'offer'].edge_index = edge_index
data['customer', 'impressed_on', 'offer'].edge_label = edge_label

print(f"HeteroData built: {data}")
print(f"CM Features: {data['customer'].x.shape}")
print(f"Offer Features: {data['offer'].x.shape}")
print(f"Edges: {data['customer', 'impressed_on', 'offer'].edge_index.shape}")
print(f"Labels: {data['customer', 'impressed_on', 'offer'].edge_label.shape}")

# --- 3. TRAIN/VAL/TEST SPLIT (on Edges) ---

# We split the *edges* (impressions) into train, validation, and test sets.
edge_indices = np.arange(NUM_IMPRESSIONS)
train_idx, temp_idx = train_test_split(edge_indices, test_size=0.2, random_state=42)
val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, random_state=42)

# Assign edge split to the data object (PyG LinkNeighborLoader uses these)
data['customer', 'impressed_on', 'offer'].edge_label_index = edge_index[:, train_idx]
data['customer', 'impressed_on', 'offer'].train_mask = torch.zeros(NUM_IMPRESSIONS, dtype=torch.bool)
data['customer', 'impressed_on', 'offer'].train_mask[train_idx] = True

# Store val and test indices separately for evaluation
val_edge_index = edge_index[:, val_idx]
val_edge_label = edge_label[val_idx]
test_edge_index = edge_index[:, test_idx]
test_edge_label = edge_label[test_idx]

# --- 4. GNN MODEL DEFINITION ---

# Define the hidden dimension for the GNN embeddings
HIDDEN_DIM = 64

class HeteroGNN(torch.nn.Module):
    def __init__(self, hidden_channels, out_channels, metadata):
        super().__init__()
        
        # 1. GNN Encoder: Message passing for one step (simplest GNN)
        # We use HeteroConv to aggregate information across the 'customer -> offer' relation.
        self.conv1 = HeteroConv({
            # The key is the relation type (src, rel, dst)
            ('customer', 'impressed_on', 'offer'): SAGEConv((-1, -1), hidden_channels),
            # Add the reverse relation for better information flow
            ('offer', 'rev_impressed_on', 'customer'): SAGEConv((-1, -1), hidden_channels),
        }, aggr='sum')
        
        # 2. Decoder: Simple MLP for link prediction
        self.lin = torch.nn.Linear(2 * hidden_channels, out_channels)

    def forward(self, x_dict, edge_index_dict):
        # 1. GNN Encoder (Generate Node Embeddings)
        # We add a reversed edge type for message passing from offer back to customer
        # Note: PyG handles the reversed mapping internally for HeteroConv during initialization
        x_dict = self.conv1(x_dict, edge_index_dict)
        x_dict = {key: F.relu(x) for key, x in x_dict.items()}
        
        # 2. Get the specific embeddings we need for the prediction
        # The training targets only involve the 'customer' and 'offer' node embeddings
        cm_emb = x_dict['customer']
        offer_emb = x_dict['offer']
        
        return cm_emb, offer_emb

class LinkPredictor(torch.nn.Module):
    # The decoder takes the embeddings of the two connected nodes and predicts the label.
    def __init__(self, hidden_channels):
        super().__init__()
        # Simple two-layer MLP for prediction
        self.lin1 = torch.nn.Linear(2 * hidden_channels, hidden_channels)
        self.lin2 = torch.nn.Linear(hidden_channels, 1)

    def forward(self, src_emb, dst_emb):
        # Concatenate the source (CM) and destination (Offer) embeddings
        x = torch.cat([src_emb, dst_emb], dim=-1)
        x = self.lin1(x)
        x = F.relu(x)
        x = self.lin2(x)
        return torch.sigmoid(x) # Output is a probability (0 to 1)

# Initialize model components
gnn_model = HeteroGNN(HIDDEN_DIM, HIDDEN_DIM, data.metadata())
predictor = LinkPredictor(HIDDEN_DIM)
optimizer = torch.optim.Adam(
    list(gnn_model.parameters()) + list(predictor.parameters()), lr=0.01)
criterion = torch.nn.BCELoss() # Binary Cross-Entropy for probability prediction

# Move data and models to GPU if available (LUMI is likely GPU based)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\nUsing device: {device}")
data = data.to(device)
gnn_model.to(device)
predictor.to(device)

# --- 5. TRAINING LOOP ---

def train(data, predictor, gnn_model, criterion, optimizer):
    gnn_model.train()
    predictor.train()
    optimizer.zero_grad()
    
    # 1. GNN Forward Pass: Compute all node embeddings
    cm_emb_all, offer_emb_all = gnn_model(data.x_dict, data.edge_index_dict)
    
    # 2. Get embeddings for the training edges
    # 'edge_label_index' holds the indices of the training edges
    edge_index = data['customer', 'impressed_on', 'offer'].edge_label_index
    
    # Map edge indices to node embeddings
    src_emb = cm_emb_all[edge_index[0]]
    dst_emb = offer_emb_all[edge_index[1]]
    
    # 3. Predict redemption probability
    pred = predictor(src_emb, dst_emb).squeeze()
    
    # 4. Calculate Loss
    target = data['customer', 'impressed_on', 'offer'].edge_label[data['customer', 'impressed_on', 'offer'].train_mask]
    loss = criterion(pred, target)
    
    # 5. Backpropagation
    loss.backward()
    optimizer.step()
    return loss.item()

@torch.no_grad()
def evaluate(split_edge_index, split_edge_label, predictor, gnn_model):
    gnn_model.eval()
    predictor.eval()

    # 1. GNN Forward Pass (on the whole graph for inference)
    cm_emb_all, offer_emb_all = gnn_model(data.x_dict, data.edge_index_dict)

    # 2. Get embeddings for the evaluation edges
    src_emb = cm_emb_all[split_edge_index[0]]
    dst_emb = offer_emb_all[split_edge_index[1]]

    # 3. Predict redemption probability
    pred = predictor(src_emb, dst_emb).squeeze()
    
    # 4. Calculate AUC-ROC
    y_true = split_edge_label.cpu().numpy()
    y_pred = pred.cpu().numpy()
    
    return roc_auc_score(y_true, y_pred)


# Run the training
EPOCHS = 5
for epoch in range(1, EPOCHS + 1):
    loss = train(data, predictor, gnn_model, criterion, optimizer)
    val_auc = evaluate(val_edge_index.to(device), val_edge_label.to(device), predictor, gnn_model)
    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val AUC: {val_auc:.4f}')

# --- 6. FINAL EVALUATION AND INFERENCE ---

print("\n--- Final Evaluation ---")
test_auc = evaluate(test_edge_index.to(device), test_edge_label.to(device), predictor, gnn_model)
print(f'Test AUC-ROC: {test_auc:.4f}')

# --- 7. INFERENCE FOR RANKING (Example) ---

@torch.no_grad()
def rank_offers_for_cm(cm_id, num_offers_to_rank):
    gnn_model.eval()
    predictor.eval()
    
    # Get all final node embeddings
    cm_emb_all, offer_emb_all = gnn_model(data.x_dict, data.edge_index_dict)
    
    # Get the embedding for the target CM (assuming cm_id is the index 0 to 110999)
    target_cm_emb = cm_emb_all[cm_id].expand(NUM_OFFERS, -1)
    
    # Predict probability for this CM against ALL offers
    offer_predictions = predictor(target_cm_emb, offer_emb_all).squeeze()
    
    # Get the top N ranked offers
    ranked_probs, ranked_offer_indices = torch.topk(offer_predictions, k=num_offers_to_rank)
    
    print(f"\nTop {num_offers_to_rank} Offers Ranked for CM Index {cm_id}:")
    for i, (prob, offer_idx) in enumerate(zip(ranked_probs.cpu().numpy(), ranked_offer_indices.cpu().numpy())):
        print(f"Rank {i+1}: Offer Index {offer_idx}, P(Redeem) = {prob:.4f}")

# Example: Rank the top 5 offers for Card Member with index 100
rank_offers_for_cm(cm_id=100, num_offers_to_rank=5)
