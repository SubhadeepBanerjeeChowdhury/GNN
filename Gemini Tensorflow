This is an exciting project! Building a Graph Neural Network (GNN) for offer personalization is a great way to leverage the relational structure between Card Members (CMs) and offers, which a standard XGBoost model might not fully capture.
​Given your constraints (2.5M rows, 123 features, specific objective, and platform), here is a basic outline and code structure for a Heterogeneous Bipartite Graph GNN using TensorFlow/Keras and the Spektral library (a popular GNN library for Keras), as it's well-suited for a Python/TensorFlow environment like a Jupyter Notebook on GCP.
​1. GNN Model Strategy: Bipartite Graph
​The most natural way to model this is as a bipartite graph where:
​One set of nodes are the Card Members (CMs), keyed by cust_xref_id.
​The other set of nodes are the Offers, keyed by offer_id.
​Edges represent the Impression Event (a CM seeing an offer). Since you have 2.5M rows of impressions, these are your edges.
​The goal is to predict the edge feature/label (Redemption, depnt_var) or, more commonly in recommendation systems, to learn node embeddings that can then be used to calculate a score (the probability of redemption) for a CM-Offer pair.
​2. Data Preparation and Graph Construction
​The key challenge is converting your tabular data into a graph format.
​Step 2.1: Initial Data Loading
​First, load your data and create unique mappings for CM and Offer IDs.




import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, QuantileTransformer

# Define your feature lists (using placeholders)
member_features = ['cm_feature_1', 'cm_feature_2', 'cm_feature_3', 'cm_feature_4', 'cm_feature_5', 'cm_feature_6', 'cm_feature_7', 'cm_feature_8', 'cm_feature_9', 'cm_feature_10']
offer_features = ['offer_feature_1', 'offer_feature_2', 'offer_feature_3', 'offer_feature_4', 'offer_feature_5', 'offer_feature_6', 'offer_feature_7', 'offer_feature_8', 'offer_feature_9', 'offer_feature_10']

# Total features to use
all_features = ['cust_xref_id', 'offer_id', 'depnt_var'] + member_features + offer_features

# Load the data (assuming the path is correct)
# NOTE: Replace with your actual file path
df = pd.read_csv('your_2.5M_data.csv', usecols=all_features)
# Convert to appropriate types for memory efficiency (important for 2.5M rows)
for col in ['cust_xref_id', 'offer_id']:
    df[col] = df[col].astype('category')
print(f"Loaded data with {len(df)} rows.")

Step 2.2: Node Feature Matrix Creation
​Create two feature matrices: one for CM nodes (X_{CM}) and one for Offer nodes (X_{Offer}). We'll use the mean of the features across all impressions for each node.

# 1. Feature Engineering/Aggregation
# Scale numerical features
scaler = StandardScaler()
df[member_features] = scaler.fit_transform(df[member_features])
df[offer_features] = scaler.fit_transform(df[offer_features])

# Aggregate features for unique CMs and Offers
cm_features_df = df.groupby('cust_xref_id')[member_features].mean().reset_index()
offer_features_df = df.groupby('offer_id')[offer_features].mean().reset_index()

# 2. Create Mappings and Matrices
# Card Member Mapping
cm_map = {id: i for i, id in enumerate(cm_features_df['cust_xref_id'].unique())}
cm_features_df['cm_idx'] = cm_features_df['cust_xref_id'].map(cm_map)
X_cm = cm_features_df.sort_values('cm_idx')[member_features].values
n_cm = X_cm.shape[0]

# Offer Mapping
offer_map = {id: i for i, id in enumerate(offer_features_df['offer_id'].unique())}
offer_features_df['offer_idx'] = offer_features_df['offer_id'].map(offer_map)
X_offer = offer_features_df.sort_values('offer_idx')[offer_features].values
n_offer = X_offer.shape[0]

# Combine all node features into one matrix X for simplicity in some GNN libs
X = np.concatenate([X_cm, X_offer], axis=0).astype('float32')
n_nodes = X.shape[0]

print(f"Total CMs: {n_cm}, Total Offers: {n_offer}, Total Nodes: {n_nodes}")



Step 2.3: Adjacency Matrix and Edge Labels
​Create the adjacency matrix A and the labels Y from the impression data


from scipy.sparse import csr_matrix

# Add index mappings back to the main DataFrame
df['src'] = df['cust_xref_id'].map(cm_map)
df['dst'] = df['offer_id'].map(offer_map) + n_cm # Offset offer indices

# Edge list for the entire graph
edges_src = df['src'].values
edges_dst = df['dst'].values
edges_label = df['depnt_var'].values.astype('float32') # Labels Y

# Adjacency Matrix A (Bipartite, only CM->Offer edges)
# The row indices are CMs, column indices are Offers (offset by n_cm)
A = csr_matrix((np.ones(len(df), dtype=np.float32), (edges_src, edges_dst)),
               shape=(n_nodes, n_nodes))
A = A + A.T # Make the adjacency matrix symmetric (for most GNN layers)

# Split data into training/testing impressions
train_df, test_df = train_test_split(df, test_size=0.1, stratify=df['depnt_var'], random_state=42)
print(f"Train impressions: {len(train_df)}, Test impressions: {len(test_df)}")

# Separate training features (indices) and labels
A_train = csr_matrix((np.ones(len(train_df), dtype=np.float32),
                      (train_df['src'].values, train_df['dst'].values)),
                     shape=(n_nodes, n_nodes))
A_train = A_train + A_train.T # Symmetric train matrix

# Training data (for link prediction)
y_train_idx = (train_df['src'].values, train_df['dst'].values)
y_train_label = train_df['depnt_var'].values.astype('float32')

# Test data (for link prediction)
y_test_idx = (test_df['src'].values, test_df['dst'].values)
y_test_label = test_df['depnt_var'].values.astype('float32')



3. GNN Model Implementation (Link Prediction)
​This model uses a Graph Convolutional Network (GCN) encoder to learn node embeddings, followed by a simple decoder (dot product or concatenation) to predict the link score (redemption probability).


import spektral
from spektral.layers import GCNConv
from spektral.data import DisjointLoader
from spektral.transforms import AdjToSpTensor

# The GNN model will learn embeddings for all CMs and Offers
# The final score is calculated only for the observed impressions (links)

# 1. Define the GNN Model Architecture (Encoder)
class LinkPredictionGNN(tf.keras.Model):
    def __init__(self, n_features, n_output_features, **kwargs):
        super().__init__(**kwargs)
        self.conv1 = GCNConv(n_output_features, activation='relu')
        self.conv2 = GCNConv(n_output_features, activation='relu') # Final node embedding size

    def call(self, inputs):
        x, a = inputs # x: node features, a: adjacency matrix
        x = self.conv1([x, a])
        x = self.conv2([x, a])
        return x # Returns the final node embeddings E

# 2. Define the Link Prediction Decoder
def decoder_dot_product(embeddings, indices):
    """Calculates the dot product between node embeddings for given indices."""
    src_emb = tf.gather(embeddings, indices[0])
    dst_emb = tf.gather(embeddings, indices[1])
    # Dot product of embeddings
    dot_prod = tf.reduce_sum(src_emb * dst_emb, axis=-1)
    return dot_prod

# 3. Custom Training Step (since we are predicting on edges, not nodes)
class LinkPredictionModel(tf.keras.Model):
    def __init__(self, gnn_encoder, **kwargs):
        super().__init__(**kwargs)
        self.gnn_encoder = gnn_encoder
        self.output_layer = tf.keras.layers.Dense(1, activation='sigmoid') # Final prediction layer

    def call(self, inputs):
        X, A, train_indices = inputs
        # 1. Get Node Embeddings E
        E = self.gnn_encoder([X, A])
        # 2. Select Embeddings for Training Links
        src_emb = tf.gather(E, train_indices[0])
        dst_emb = tf.gather(E, train_indices[1])
        # 3. Concatenate and Decode (or use dot product)
        concat_emb = tf.concat([src_emb, dst_emb], axis=-1)
        score = self.output_layer(concat_emb)
        return tf.squeeze(score, axis=-1)

# 4. Model Instantiation and Compilation
NODE_EMBEDDING_SIZE = 64

gnn_encoder = LinkPredictionGNN(X.shape[1], NODE_EMBEDDING_SIZE)
model = LinkPredictionModel(gnn_encoder)

# The model needs to be compiled with a loss and metric
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss=tf.keras.losses.BinaryCrossentropy(),
    metrics=[tf.keras.metrics.AUC(name='auc')]
)

# Convert sparse matrices and indices to tensors
X_tensor = tf.convert_to_tensor(X, dtype=tf.float32)
A_train_tensor = AdjToSpTensor()(A_train) # Convert to Sparse Tensor
y_train_idx_tensor = (tf.constant(y_train_idx[0]), tf.constant(y_train_idx[1]))
y_test_idx_tensor = (tf.constant(y_test_idx[0]), tf.constant(y_test_idx[1]))

# Prepare inputs for training
train_inputs = (X_tensor, A_train_tensor, y_train_idx_tensor)
train_labels = y_train_label

# 5. Training the Model
print("Starting GNN training...")
history = model.fit(
    train_inputs,
    train_labels,
    epochs=10, # Start with a small number of epochs
    batch_size=len(y_train_label), # Full-batch training on the graph
    validation_data=((X_tensor, A_train_tensor, y_test_idx_tensor), y_test_label)
)
print("Training complete.")

# NOTE on LUMI/GCP: Ensure you are utilizing the available GPU/TPU resources by
# wrapping your model creation and fitting in a tf.distribute.Strategy scope.












4. Generating Predictions and Ranking
​After training, you can use the model to generate the probability of redemption for any CM-Offer pair, including those not seen in the training data (a key benefit of GNNs).



# 1. Get the final node embeddings from the trained encoder
final_embeddings = model.gnn_encoder([X_tensor, A_train_tensor])

# 2. Predict on the Test Set
test_predictions = model((X_tensor, A_train_tensor, y_test_idx_tensor))
test_auc = tf.keras.metrics.AUC()
test_auc.update_state(y_test_label, test_predictions.numpy())
print(f"\nTest AUC: {test_auc.result().numpy():.4f}")

# 3. Generate New Predictions for a Target CM (Personalization/Ranking)
# Example: Select a target CM and all offers
TARGET_CM_XREF_ID = df['cust_xref_id'].sample(1).iloc[0] # Pick a random CM
target_cm_idx = cm_map[TARGET_CM_XREF_ID]

# Create new prediction indices for the target CM with ALL offers
all_offer_indices = np.arange(n_offer) + n_cm
target_cm_indices = np.full(n_offer, target_cm_idx)

new_prediction_indices = (tf.constant(target_cm_indices), tf.constant(all_offer_indices))

# The model's `call` method can be used directly for prediction
# Pass the full graph structure (X, A_train) and the new indices
probabilities = model((X_tensor, A_train_tensor, new_prediction_indices)).numpy()

# 4. Create Ranking
offer_id_list = list(offer_map.keys())
ranking_df = pd.DataFrame({
    'offer_id': offer_id_list,
    'P_Redeem': probabilities
})

ranking_df = ranking_df.sort_values(by='P_Redeem', ascending=False)
print(f"\nTop 5 Offer Recommendations for CM: {TARGET_CM_XREF_ID}")
print(ranking_df.head(5))












